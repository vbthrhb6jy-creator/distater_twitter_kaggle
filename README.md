# distater_twitter_kaggle
Decipher tweets are disaster related or not using RNN methods
WRITE UP

STEP 1: The problem is to use a matrix factoriziation and a sequential nueral network application to classify tweets into disaster related tweets or not disster related tweets. We will be training on about 7k tweets to then predict aout 3200. There are some identical helper columns with keyword and location to help classify.

Step 2 EDA, I did some regular info and head on the test and train data. Then created some histograms and stacked bars centered around the keywords as well as locations to see which keywords or locations lean towards others. I also performed some stop words by grabbing the most common. My plan is to remove a reasonable amount of stop words. before pushing it through the RNN.

STEP 3: for model architecture post scrubbing stop words...Post tokenizing at 50 tokens per tweet and with a mini word2vec formula using a normal distribution to vectorize the tokens and to factorize the data and add 100 dimensions. I realize that 50 tokens were way too many given the sizes of most tweets and that 50 dimensions was too many. I then went down to 15 and saw a huge leap in performace. After tokenizing and vectorizing i built a bi directional GRU model with only the vectorized text. In all models I played with different return sequences gathering either each step or just the final thought as a whole. I also tired to add the keywords to the model by transforming them into a number and concatenating them in the end of the model before the result.

STEP 4 my after fitting to just the cleaned & vectorizezd train text i saw a very tight band of my sigmoid acitvation. I learned that this is because i had too many zeros in the tokenizing part. After shorting the tokens I saw my accuracy go up to about 2/3. which i wanted to improve. I experimented with longer epochs which saw some marginal improvemnts. Then I attempeted to assign numerical values to the keywords and locations as those also could give direction to whether a tweet was a disaster or not. This was tough and had to concatenate after a few layers of the model. This however did increase my accuracy quite a bit to 74%

Step 5: I think having a digestable number of parameters is key. When i have too many and learning rate is slow it takes the model a long time to improve and eats away and my cpu & gpu. I learned that having a validation data set is also beneficial. As my model would have just fit tighter and tighter but having an early stop on the validation data prevented an over fit and potentially bad test results. Also using drop out and fewer tokens /due to size of tweets greatly improved run time and accuracy. I think starting with a smaller model (in parameters) would help me learn more about what improvements i could make instead of having a large churn and losing hours while model trying to converage.
